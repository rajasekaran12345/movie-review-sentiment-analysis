{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "546e85e4",
      "metadata": {
        "id": "546e85e4"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "train_imdb_models.py\n",
        "--------------------\n",
        "Data preprocessing + training multiple RNN-based models (SimpleRNN, LSTM, Bidirectional LSTM)\n",
        "with basic hyperparameter tuning over batch size and loss function.\n",
        "\n",
        "Requirements:\n",
        "  - Python 3.8+\n",
        "  - tensorflow==2.15.0\n",
        "  - (optional) numpy\n",
        "\n",
        "Notes:\n",
        "  * Final layer uses a sigmoid activation (as requested).\n",
        "  * Loss options include \"binary_crossentropy\" and \"hinge\". For \"hinge\", labels are mapped to {-1, +1}.\n",
        "    Using \"hinge\" with a sigmoid output is acceptable but not theoretically perfect; it's included\n",
        "    to satisfy the request for tuning loss functions.\n",
        "  * Models are saved in .h5 format.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "59535d71",
      "metadata": {
        "id": "59535d71"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ca9e0aaa",
      "metadata": {
        "id": "ca9e0aaa"
      },
      "outputs": [],
      "source": [
        "# Config\n",
        "VOCAB_SIZE = 20000        # top words to consider\n",
        "MAXLEN = 500              # cut reviews after this number of words\n",
        "EMBEDDING_DIM = 128\n",
        "EPOCHS_TUNE = 3           # small number for quick tuning\n",
        "EPOCHS_FINAL = 6          # longer training with best settings\n",
        "VALIDATION_SPLIT = 0.2\n",
        "OUTPUT_DIR = \"models\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "71ddc0a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71ddc0a0",
        "outputId": "55188b06-a3b4-4b8f-bff9-7a9b60ec0be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading IMDB dataset…\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "[INFO] Padding sequences…\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "print(\"[INFO] Loading IMDB dataset…\")\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=VOCAB_SIZE)\n",
        "\n",
        "print(\"[INFO] Padding sequences…\")\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=MAXLEN)\n",
        "x_test  = sequence.pad_sequences(x_test,  maxlen=MAXLEN)\n",
        "\n",
        "# For hinge loss, labels should be in {-1, +1}\n",
        "y_train_hinge = (y_train * 2 - 1).astype(np.float32)\n",
        "y_test_hinge  = (y_test  * 2 - 1).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "689d0796",
      "metadata": {
        "id": "689d0796"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Bidirectional, Dense, Dropout\n",
        "\n",
        "def build_simple_rnn_model():\n",
        "    model = Sequential([\n",
        "        Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAXLEN),\n",
        "        SimpleRNN(128),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_lstm_model():\n",
        "    model = Sequential([\n",
        "        Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAXLEN),\n",
        "        LSTM(128),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_bi_lstm_model():\n",
        "    model = Sequential([\n",
        "        Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAXLEN),\n",
        "        Bidirectional(LSTM(128)),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "MODEL_BUILDERS = {\n",
        "    \"simple_rnn\": build_simple_rnn_model,\n",
        "    \"lstm\": build_lstm_model,\n",
        "    \"bilstm\": build_bi_lstm_model,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "27e78cfd",
      "metadata": {
        "id": "27e78cfd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "batch_sizes = [32, 64, 128]\n",
        "loss_options = [\"binary_crossentropy\", \"hinge\"]\n",
        "\n",
        "def compile_model(model, loss_name):\n",
        "    if loss_name == \"binary_crossentropy\":\n",
        "        loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "        metrics = [\"accuracy\"]\n",
        "    elif loss_name == \"hinge\":\n",
        "        loss = tf.keras.losses.Hinge()\n",
        "        metrics = [\"accuracy\"]\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported loss: {loss_name}\")\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-3), loss=loss, metrics=metrics)\n",
        "    return model\n",
        "\n",
        "def get_labels_for_loss(loss_name, train=True):\n",
        "    if loss_name == \"binary_crossentropy\":\n",
        "        return (y_train if train else y_test)\n",
        "    elif loss_name == \"hinge\":\n",
        "        return (y_train_hinge if train else y_test_hinge)\n",
        "    else:\n",
        "        raise ValueError(loss_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "db3bd73c",
      "metadata": {
        "id": "db3bd73c"
      },
      "outputs": [],
      "source": [
        "def tune_for_model(model_name):\n",
        "    print(f\"\\n[INFO] Tuning hyperparameters for: {model_name}\")\n",
        "    best_cfg = None\n",
        "    best_val = -np.inf\n",
        "\n",
        "    for loss_name in loss_options:\n",
        "        y_tr = get_labels_for_loss(loss_name, train=True)\n",
        "        for bs in batch_sizes:\n",
        "            model = MODEL_BUILDERS[model_name]()\n",
        "            compile_model(model, loss_name)\n",
        "            callbacks = [EarlyStopping(patience=2, restore_best_weights=True, monitor=\"val_accuracy\")]\n",
        "            hist = model.fit(\n",
        "                x_train, y_tr,\n",
        "                batch_size=bs,\n",
        "                epochs=EPOCHS_TUNE,\n",
        "                validation_split=VALIDATION_SPLIT,\n",
        "                verbose=2,\n",
        "                callbacks=callbacks\n",
        "            )\n",
        "            val_acc = max(hist.history.get(\"val_accuracy\", [0]))\n",
        "            print(f\"[TUNE] model={model_name} loss={loss_name} batch_size={bs} -> best val_acc={val_acc:.4f}\")\n",
        "            if val_acc > best_val:\n",
        "                best_val = val_acc\n",
        "                best_cfg = {\"loss\": loss_name, \"batch_size\": bs}\n",
        "    print(f\"[BEST] {model_name}: {best_cfg} (val_acc={best_val:.4f})\")\n",
        "    return best_cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "65924350",
      "metadata": {
        "id": "65924350"
      },
      "outputs": [],
      "source": [
        "def train_and_save(model_name, best_cfg):\n",
        "    loss_name = best_cfg[\"loss\"]\n",
        "    batch_size = best_cfg[\"batch_size\"]\n",
        "    y_tr = get_labels_for_loss(loss_name, train=True)\n",
        "    y_te = get_labels_for_loss(loss_name, train=False)\n",
        "\n",
        "    model = MODEL_BUILDERS[model_name]()\n",
        "    compile_model(model, loss_name)\n",
        "\n",
        "    callbacks = [EarlyStopping(patience=3, restore_best_weights=True, monitor=\"val_accuracy\")]\n",
        "\n",
        "    print(f\"[INFO] Training final {model_name} with loss={loss_name}, batch_size={batch_size}\")\n",
        "    history = model.fit(\n",
        "        x_train, y_tr,\n",
        "        batch_size=batch_size,\n",
        "        epochs=EPOCHS_FINAL,\n",
        "        validation_split=VALIDATION_SPLIT,\n",
        "        verbose=2,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    print(\"[INFO] Evaluating on test set…\")\n",
        "    test_metrics = model.evaluate(x_test, y_te, verbose=0)\n",
        "    print(f\"[RESULT] {model_name} test -> loss={test_metrics[0]:.4f}, acc={test_metrics[1]:.4f}\")\n",
        "\n",
        "    out_path = os.path.join(OUTPUT_DIR, f\"{model_name}_imdb.h5\")\n",
        "    model.save(out_path)\n",
        "    print(f\"[SAVED] {out_path}\")\n",
        "    return out_path, history.history, test_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3e94345f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e94345f",
        "outputId": "cacddbd9-6ff6-4b91-ecda-e8535375e416"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[INFO] Tuning hyperparameters for: simple_rnn\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "625/625 - 30s - 48ms/step - accuracy: 0.6003 - loss: 0.6643 - val_accuracy: 0.5514 - val_loss: 0.6782\n",
            "Epoch 2/3\n",
            "625/625 - 24s - 39ms/step - accuracy: 0.6209 - loss: 0.6463 - val_accuracy: 0.6362 - val_loss: 0.6292\n",
            "Epoch 3/3\n",
            "625/625 - 42s - 67ms/step - accuracy: 0.7254 - loss: 0.5300 - val_accuracy: 0.6398 - val_loss: 0.6268\n",
            "[TUNE] model=simple_rnn loss=binary_crossentropy batch_size=32 -> best val_acc=0.6398\n",
            "Epoch 1/3\n",
            "313/313 - 18s - 58ms/step - accuracy: 0.5277 - loss: 0.7083 - val_accuracy: 0.6088 - val_loss: 0.6566\n",
            "Epoch 2/3\n",
            "313/313 - 17s - 56ms/step - accuracy: 0.6812 - loss: 0.5833 - val_accuracy: 0.7168 - val_loss: 0.5541\n",
            "Epoch 3/3\n",
            "313/313 - 20s - 64ms/step - accuracy: 0.7976 - loss: 0.4453 - val_accuracy: 0.7412 - val_loss: 0.5469\n",
            "[TUNE] model=simple_rnn loss=binary_crossentropy batch_size=64 -> best val_acc=0.7412\n",
            "Epoch 1/3\n",
            "157/157 - 14s - 87ms/step - accuracy: 0.5643 - loss: 0.6748 - val_accuracy: 0.6732 - val_loss: 0.5906\n",
            "Epoch 2/3\n",
            "157/157 - 7s - 47ms/step - accuracy: 0.7787 - loss: 0.4710 - val_accuracy: 0.7676 - val_loss: 0.5011\n",
            "Epoch 3/3\n",
            "157/157 - 7s - 47ms/step - accuracy: 0.8632 - loss: 0.3229 - val_accuracy: 0.7794 - val_loss: 0.4983\n",
            "[TUNE] model=simple_rnn loss=binary_crossentropy batch_size=128 -> best val_acc=0.7794\n",
            "Epoch 1/3\n",
            "625/625 - 28s - 45ms/step - accuracy: 0.1230 - loss: 0.9974 - val_accuracy: 0.0880 - val_loss: 0.9716\n",
            "Epoch 2/3\n",
            "625/625 - 39s - 63ms/step - accuracy: 0.3192 - loss: 0.8802 - val_accuracy: 0.2820 - val_loss: 0.8923\n",
            "Epoch 3/3\n",
            "625/625 - 41s - 65ms/step - accuracy: 0.3697 - loss: 0.7665 - val_accuracy: 0.3178 - val_loss: 0.8375\n",
            "[TUNE] model=simple_rnn loss=hinge batch_size=32 -> best val_acc=0.3178\n",
            "Epoch 1/3\n",
            "313/313 - 19s - 61ms/step - accuracy: 0.3110 - loss: 0.9901 - val_accuracy: 0.4778 - val_loss: 0.9941\n",
            "Epoch 2/3\n",
            "313/313 - 13s - 41ms/step - accuracy: 0.3163 - loss: 0.9334 - val_accuracy: 0.4790 - val_loss: 1.0058\n",
            "Epoch 3/3\n",
            "313/313 - 21s - 67ms/step - accuracy: 0.4385 - loss: 0.8960 - val_accuracy: 0.4648 - val_loss: 0.9350\n",
            "[TUNE] model=simple_rnn loss=hinge batch_size=64 -> best val_acc=0.4790\n",
            "Epoch 1/3\n",
            "157/157 - 12s - 79ms/step - accuracy: 0.2254 - loss: 0.9957 - val_accuracy: 0.4856 - val_loss: 0.9991\n",
            "Epoch 2/3\n",
            "157/157 - 17s - 111ms/step - accuracy: 0.3011 - loss: 0.9396 - val_accuracy: 0.3532 - val_loss: 0.8981\n",
            "Epoch 3/3\n",
            "157/157 - 10s - 65ms/step - accuracy: 0.3670 - loss: 0.8077 - val_accuracy: 0.3450 - val_loss: 0.8052\n",
            "[TUNE] model=simple_rnn loss=hinge batch_size=128 -> best val_acc=0.4856\n",
            "[BEST] simple_rnn: {'loss': 'binary_crossentropy', 'batch_size': 128} (val_acc=0.7794)\n",
            "[INFO] Training final simple_rnn with loss=binary_crossentropy, batch_size=128\n",
            "Epoch 1/6\n",
            "157/157 - 14s - 87ms/step - accuracy: 0.5364 - loss: 0.6978 - val_accuracy: 0.6268 - val_loss: 0.6511\n",
            "Epoch 2/6\n",
            "157/157 - 7s - 48ms/step - accuracy: 0.6274 - loss: 0.6347 - val_accuracy: 0.7628 - val_loss: 0.5025\n",
            "Epoch 3/6\n",
            "157/157 - 10s - 66ms/step - accuracy: 0.7771 - loss: 0.4773 - val_accuracy: 0.7210 - val_loss: 0.5548\n",
            "Epoch 4/6\n",
            "157/157 - 7s - 48ms/step - accuracy: 0.7807 - loss: 0.4688 - val_accuracy: 0.8050 - val_loss: 0.4601\n",
            "Epoch 5/6\n",
            "157/157 - 10s - 66ms/step - accuracy: 0.8598 - loss: 0.3362 - val_accuracy: 0.7482 - val_loss: 0.5375\n",
            "Epoch 6/6\n",
            "157/157 - 10s - 65ms/step - accuracy: 0.8636 - loss: 0.3266 - val_accuracy: 0.7750 - val_loss: 0.5248\n",
            "[INFO] Evaluating on test set…\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[RESULT] simple_rnn test -> loss=0.4537, acc=0.8065\n",
            "[SAVED] models/simple_rnn_imdb.h5\n",
            "\n",
            "[INFO] Tuning hyperparameters for: lstm\n",
            "Epoch 1/3\n",
            "625/625 - 19s - 31ms/step - accuracy: 0.7682 - loss: 0.4744 - val_accuracy: 0.8278 - val_loss: 0.3823\n",
            "Epoch 2/3\n",
            "625/625 - 13s - 20ms/step - accuracy: 0.8897 - loss: 0.2837 - val_accuracy: 0.8354 - val_loss: 0.3947\n",
            "Epoch 3/3\n",
            "625/625 - 21s - 33ms/step - accuracy: 0.9232 - loss: 0.2087 - val_accuracy: 0.7468 - val_loss: 0.4854\n",
            "[TUNE] model=lstm loss=binary_crossentropy batch_size=32 -> best val_acc=0.8354\n",
            "Epoch 1/3\n",
            "313/313 - 11s - 34ms/step - accuracy: 0.7581 - loss: 0.4782 - val_accuracy: 0.8096 - val_loss: 0.4185\n",
            "Epoch 2/3\n",
            "313/313 - 8s - 27ms/step - accuracy: 0.9058 - loss: 0.2388 - val_accuracy: 0.8478 - val_loss: 0.3340\n",
            "Epoch 3/3\n",
            "313/313 - 10s - 33ms/step - accuracy: 0.9332 - loss: 0.1832 - val_accuracy: 0.8444 - val_loss: 0.3892\n",
            "[TUNE] model=lstm loss=binary_crossentropy batch_size=64 -> best val_acc=0.8478\n",
            "Epoch 1/3\n",
            "157/157 - 8s - 52ms/step - accuracy: 0.7447 - loss: 0.4998 - val_accuracy: 0.8552 - val_loss: 0.3473\n",
            "Epoch 2/3\n",
            "157/157 - 6s - 38ms/step - accuracy: 0.8955 - loss: 0.2673 - val_accuracy: 0.8470 - val_loss: 0.3644\n",
            "Epoch 3/3\n",
            "157/157 - 10s - 66ms/step - accuracy: 0.9354 - loss: 0.1815 - val_accuracy: 0.8288 - val_loss: 0.4410\n",
            "[TUNE] model=lstm loss=binary_crossentropy batch_size=128 -> best val_acc=0.8552\n",
            "Epoch 1/3\n",
            "625/625 - 16s - 26ms/step - accuracy: 0.3791 - loss: 0.9003 - val_accuracy: 0.4938 - val_loss: 1.0124\n",
            "Epoch 2/3\n",
            "625/625 - 19s - 30ms/step - accuracy: 0.5016 - loss: 0.9969 - val_accuracy: 0.4938 - val_loss: 1.0124\n",
            "Epoch 3/3\n",
            "625/625 - 21s - 33ms/step - accuracy: 0.5016 - loss: 0.9969 - val_accuracy: 0.4938 - val_loss: 1.0124\n",
            "[TUNE] model=lstm loss=hinge batch_size=32 -> best val_acc=0.4938\n",
            "Epoch 1/3\n",
            "313/313 - 10s - 33ms/step - accuracy: 0.3575 - loss: 0.7956 - val_accuracy: 0.4424 - val_loss: 0.7231\n",
            "Epoch 2/3\n",
            "313/313 - 10s - 32ms/step - accuracy: 0.1068 - loss: 0.9138 - val_accuracy: 0.0052 - val_loss: 0.9952\n",
            "Epoch 3/3\n",
            "313/313 - 8s - 27ms/step - accuracy: 0.0120 - loss: 0.9883 - val_accuracy: 0.0132 - val_loss: 0.9890\n",
            "[TUNE] model=lstm loss=hinge batch_size=64 -> best val_acc=0.4424\n",
            "Epoch 1/3\n",
            "157/157 - 8s - 53ms/step - accuracy: 0.4080 - loss: 0.9403 - val_accuracy: 0.4434 - val_loss: 0.8253\n",
            "Epoch 2/3\n",
            "157/157 - 6s - 38ms/step - accuracy: 0.4593 - loss: 0.9137 - val_accuracy: 0.4808 - val_loss: 1.0048\n",
            "Epoch 3/3\n",
            "157/157 - 10s - 65ms/step - accuracy: 0.4879 - loss: 0.9947 - val_accuracy: 0.4808 - val_loss: 1.0048\n",
            "[TUNE] model=lstm loss=hinge batch_size=128 -> best val_acc=0.4808\n",
            "[BEST] lstm: {'loss': 'binary_crossentropy', 'batch_size': 128} (val_acc=0.8552)\n",
            "[INFO] Training final lstm with loss=binary_crossentropy, batch_size=128\n",
            "Epoch 1/6\n",
            "157/157 - 8s - 52ms/step - accuracy: 0.7552 - loss: 0.4913 - val_accuracy: 0.8224 - val_loss: 0.3987\n",
            "Epoch 2/6\n",
            "157/157 - 10s - 64ms/step - accuracy: 0.8818 - loss: 0.3012 - val_accuracy: 0.8428 - val_loss: 0.3694\n",
            "Epoch 3/6\n",
            "157/157 - 10s - 65ms/step - accuracy: 0.9082 - loss: 0.2431 - val_accuracy: 0.8282 - val_loss: 0.4040\n",
            "Epoch 4/6\n",
            "157/157 - 10s - 66ms/step - accuracy: 0.9334 - loss: 0.1888 - val_accuracy: 0.8534 - val_loss: 0.3978\n",
            "Epoch 5/6\n",
            "157/157 - 10s - 65ms/step - accuracy: 0.9560 - loss: 0.1292 - val_accuracy: 0.8506 - val_loss: 0.4239\n",
            "Epoch 6/6\n",
            "157/157 - 6s - 39ms/step - accuracy: 0.9425 - loss: 0.1550 - val_accuracy: 0.8464 - val_loss: 0.4069\n",
            "[INFO] Evaluating on test set…\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RESULT] lstm test -> loss=0.4039, acc=0.8516\n",
            "[SAVED] models/lstm_imdb.h5\n",
            "\n",
            "[INFO] Tuning hyperparameters for: bilstm\n",
            "Epoch 1/3\n",
            "625/625 - 25s - 40ms/step - accuracy: 0.7760 - loss: 0.4657 - val_accuracy: 0.8438 - val_loss: 0.3689\n",
            "Epoch 2/3\n",
            "625/625 - 41s - 65ms/step - accuracy: 0.9006 - loss: 0.2640 - val_accuracy: 0.8652 - val_loss: 0.3420\n",
            "Epoch 3/3\n",
            "625/625 - 41s - 66ms/step - accuracy: 0.9162 - loss: 0.2216 - val_accuracy: 0.8636 - val_loss: 0.3713\n",
            "[TUNE] model=bilstm loss=binary_crossentropy batch_size=32 -> best val_acc=0.8652\n",
            "Epoch 1/3\n",
            "313/313 - 18s - 58ms/step - accuracy: 0.7417 - loss: 0.5142 - val_accuracy: 0.8548 - val_loss: 0.3536\n",
            "Epoch 2/3\n",
            "313/313 - 21s - 66ms/step - accuracy: 0.8999 - loss: 0.2565 - val_accuracy: 0.8752 - val_loss: 0.3124\n",
            "Epoch 3/3\n",
            "313/313 - 20s - 64ms/step - accuracy: 0.9387 - loss: 0.1691 - val_accuracy: 0.8638 - val_loss: 0.3462\n",
            "[TUNE] model=bilstm loss=binary_crossentropy batch_size=64 -> best val_acc=0.8752\n",
            "Epoch 1/3\n",
            "157/157 - 14s - 88ms/step - accuracy: 0.6967 - loss: 0.5554 - val_accuracy: 0.8216 - val_loss: 0.4069\n",
            "Epoch 2/3\n",
            "157/157 - 20s - 128ms/step - accuracy: 0.8619 - loss: 0.3271 - val_accuracy: 0.8638 - val_loss: 0.3238\n",
            "Epoch 3/3\n",
            "157/157 - 11s - 73ms/step - accuracy: 0.9190 - loss: 0.2165 - val_accuracy: 0.8540 - val_loss: 0.3667\n",
            "[TUNE] model=bilstm loss=binary_crossentropy batch_size=128 -> best val_acc=0.8638\n",
            "Epoch 1/3\n",
            "625/625 - 26s - 42ms/step - accuracy: 0.2048 - loss: 0.8942 - val_accuracy: 0.0216 - val_loss: 0.9976\n",
            "Epoch 2/3\n",
            "625/625 - 23s - 36ms/step - accuracy: 0.0259 - loss: 0.9906 - val_accuracy: 0.0216 - val_loss: 0.9976\n",
            "Epoch 3/3\n",
            "625/625 - 41s - 66ms/step - accuracy: 0.0259 - loss: 0.9905 - val_accuracy: 0.0216 - val_loss: 0.9976\n",
            "[TUNE] model=bilstm loss=hinge batch_size=32 -> best val_acc=0.0216\n",
            "Epoch 1/3\n",
            "313/313 - 18s - 59ms/step - accuracy: 0.3422 - loss: 0.9080 - val_accuracy: 0.4858 - val_loss: 1.0074\n",
            "Epoch 2/3\n",
            "313/313 - 15s - 49ms/step - accuracy: 0.4929 - loss: 0.9947 - val_accuracy: 0.4858 - val_loss: 1.0074\n",
            "Epoch 3/3\n",
            "313/313 - 15s - 49ms/step - accuracy: 0.4929 - loss: 0.9946 - val_accuracy: 0.4858 - val_loss: 1.0074\n",
            "[TUNE] model=bilstm loss=hinge batch_size=64 -> best val_acc=0.4858\n",
            "Epoch 1/3\n",
            "157/157 - 14s - 92ms/step - accuracy: 0.2726 - loss: 0.9289 - val_accuracy: 0.3140 - val_loss: 0.7719\n",
            "Epoch 2/3\n",
            "157/157 - 11s - 72ms/step - accuracy: 0.4075 - loss: 0.7462 - val_accuracy: 0.3894 - val_loss: 0.7128\n",
            "Epoch 3/3\n",
            "157/157 - 11s - 73ms/step - accuracy: 0.4261 - loss: 0.6560 - val_accuracy: 0.3826 - val_loss: 0.6888\n",
            "[TUNE] model=bilstm loss=hinge batch_size=128 -> best val_acc=0.3894\n",
            "[BEST] bilstm: {'loss': 'binary_crossentropy', 'batch_size': 64} (val_acc=0.8752)\n",
            "[INFO] Training final bilstm with loss=binary_crossentropy, batch_size=64\n",
            "Epoch 1/6\n",
            "313/313 - 18s - 59ms/step - accuracy: 0.7715 - loss: 0.4656 - val_accuracy: 0.8650 - val_loss: 0.3244\n",
            "Epoch 2/6\n",
            "313/313 - 20s - 65ms/step - accuracy: 0.9006 - loss: 0.2603 - val_accuracy: 0.8736 - val_loss: 0.3177\n",
            "Epoch 3/6\n",
            "313/313 - 21s - 68ms/step - accuracy: 0.9273 - loss: 0.1976 - val_accuracy: 0.8526 - val_loss: 0.3685\n",
            "Epoch 4/6\n",
            "313/313 - 20s - 65ms/step - accuracy: 0.9462 - loss: 0.1476 - val_accuracy: 0.8150 - val_loss: 0.4325\n",
            "Epoch 5/6\n",
            "313/313 - 15s - 48ms/step - accuracy: 0.9563 - loss: 0.1242 - val_accuracy: 0.8636 - val_loss: 0.4322\n",
            "[INFO] Evaluating on test set…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RESULT] bilstm test -> loss=0.3376, acc=0.8652\n",
            "[SAVED] models/bilstm_imdb.h5\n",
            "\n",
            "=== TRAINING SUMMARY ===\n",
            "simple_rnn: {'best_cfg': {'loss': 'binary_crossentropy', 'batch_size': 128}, 'model_path': 'models/simple_rnn_imdb.h5', 'history_keys': ['accuracy', 'loss', 'val_accuracy', 'val_loss'], 'test_loss': 0.45372241735458374, 'test_acc': 0.8065199851989746}\n",
            "lstm: {'best_cfg': {'loss': 'binary_crossentropy', 'batch_size': 128}, 'model_path': 'models/lstm_imdb.h5', 'history_keys': ['accuracy', 'loss', 'val_accuracy', 'val_loss'], 'test_loss': 0.40390223264694214, 'test_acc': 0.8515599966049194}\n",
            "bilstm: {'best_cfg': {'loss': 'binary_crossentropy', 'batch_size': 64}, 'model_path': 'models/bilstm_imdb.h5', 'history_keys': ['accuracy', 'loss', 'val_accuracy', 'val_loss'], 'test_loss': 0.3375725746154785, 'test_acc': 0.8651599884033203}\n",
            "\n",
            "All done. Models saved under: models\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    summaries = {}\n",
        "    for name in MODEL_BUILDERS.keys():\n",
        "        best_cfg = tune_for_model(name)\n",
        "        path, hist, test_metrics = train_and_save(name, best_cfg)\n",
        "        summaries[name] = {\n",
        "            \"best_cfg\": best_cfg,\n",
        "            \"model_path\": path,\n",
        "            \"history_keys\": list(hist.keys()),\n",
        "            \"test_loss\": float(test_metrics[0]),\n",
        "            \"test_acc\": float(test_metrics[1]),\n",
        "        }\n",
        "\n",
        "    print(\"\\n=== TRAINING SUMMARY ===\")\n",
        "    for k, v in summaries.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "    print(\"\\nAll done. Models saved under:\", OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pd_qjYr4x3Ts"
      },
      "id": "pd_qjYr4x3Ts",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}